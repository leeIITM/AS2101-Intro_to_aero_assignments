# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W29d9qvIj9pnrJnCw7FqVnysxqhyPCC2
"""

data1="/content/drive/MyDrive/data1.txt"
data2="/content/drive/MyDrive/data2.txt"
data3="/content/drive/MyDrive/data3.txt"

import numpy as np

# Open and read the file
with open( data1, 'r') as file:
    data = file.readlines()

# Initialize empty lists for x and y
x_values = []
y_values = []

# Split each line by space and store x and y values
for line in data:
    x, y = map(float, line.split())
    x_values.append(x)
    y_values.append(y)

# Convert lists to numpy arrays
x1_array = np.array(x_values)
y1_array = np.array(y_values)

with open( data2, 'r') as file:
    data = file.readlines()

# Initialize empty lists for x and y
x_values = []
y_values = []

# Split each line by space and store x and y values
for line in data:
    x, y = map(float, line.split())
    x_values.append(x)
    y_values.append(y)

# Convert lists to numpy arrays
x2_array = np.array(x_values)
y2_array = np.array(y_values)

with open( data3, 'r') as file:
    data = file.readlines()

# Initialize empty lists for x and y
x_values = []
y_values = []

# Split each line by space and store x and y values
for line in data:
    x, y = map(float, line.split())
    x_values.append(x)
    y_values.append(y)

# Convert lists to numpy arrays
x3_array = np.array(x_values)
y3_array = np.array(y_values)

import matplotlib.pyplot as plt
import numpy as np
import matplotlib.pyplot as plt

import numpy as np
import matplotlib.pyplot as plt

def gram_schmidt(X):

    m, n = X.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    for j in range(n):
        # Compute the norm and update Q
        v = X[:, j]
        for i in range(j):
            R[i, j] = np.dot(Q[:, i].T, v)
            v = v - R[i, j] * Q[:, i]
        R[j, j] = np.linalg.norm(v)
        Q[:, j] = v / R[j, j]

    return Q, R

def back_substitution(R, QTy):

    n = R.shape[1]
    beta = np.zeros(n)

    for i in range(n - 1, -1, -1):
        beta[i] = (QTy[i] - np.dot(R[i, i+1:], beta[i+1:])) / R[i, i]

    return beta

def QR_decomposition(X, y):

    # Add the intercept term (column of ones) to X
    X_b = np.c_[np.ones((X.shape[0], 1)), X]

    # Perform QR decomposition using Gram-Schmidt
    Q, R = gram_schmidt(X_b)

    # Compute Q.T * y
    QTy = np.dot(Q.T, y)

    # Solve for the regression coefficients (beta) using back substitution
    beta = back_substitution(R, QTy)

    # Print the coefficients (theta)
    print(f"Intercept: {beta[0]}, Slope: {beta[1]}")

    # Plot the data and the regression line
    plt.scatter(X, y, color='blue', label='Data points')
    plt.plot(X, X_b.dot(beta), color='red', label='Regression line')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.show()

    return beta

import numpy as np
import matplotlib.pyplot as plt

def gram_schmidt(X):

    m, n = X.shape
    Q = np.zeros((m, n))
    R = np.zeros((n, n))

    for j in range(n):
        # Compute the norm and update Q
        v = X[:, j]
        for i in range(j):
            R[i, j] = np.dot(Q[:, i].T, v)
            v = v - R[i, j] * Q[:, i]
        R[j, j] = np.linalg.norm(v)
        Q[:, j] = v / R[j, j]

    return Q, R

def back_substitution(R, QTy):

    n = R.shape[1]
    beta = np.zeros(n)

    for i in range(n - 1, -1, -1):
        beta[i] = (QTy[i] - np.dot(R[i, i+1:], beta[i+1:])) / R[i, i]

    return beta

def QR_decomposition(X, y, degree=1):

    # Adding polynomial terms (degree 1 is linear, degree 2 is quadratic, degree 3 is cubic, etc.)
    X_b = np.ones((X.shape[0], 1))  # Start with the intercept term
    for d in range(1, degree + 1):
        X_b = np.c_[X_b, X**d]

    # Performing QR decomposition using Gram-Schmidt
    Q, R = gram_schmidt(X_b)

    # Compute Q.T * y
    QTy = np.dot(Q.T, y)

    # Solving for the regression coefficients (beta) using back substitution
    beta = back_substitution(R, QTy)

    # Printing the coefficients
    print(f"Coefficients: {beta}")

    # Plot the data and the polynomial regression line
    plt.scatter(X, y, color='blue', label='Data points')

    # Plot the fitted polynomial curve
    X_range = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)
    X_range_b = np.ones((X_range.shape[0], 1))  # Start with intercept term
    for d in range(1, degree + 1):
        X_range_b = np.c_[X_range_b, X_range**d]

    plt.plot(X_range, X_range_b.dot(beta), color='red', label=f'{degree}-degree Polynomial Fit')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.show()

    return beta
# For x1_array, y1_array
print("Linear Regression for data1 (x1_array, y1_array)")
beta_linear = QR_decomposition(x1_array, y1_array, degree=1)

print("Quadratic Regression for data1 (x1_array, y1_array)")
beta_quadratic = QR_decomposition(x1_array, y1_array, degree=2)

print("Cubic Regression for data1 (x1_array, y1_array)")
beta_cubic = QR_decomposition(x1_array, y1_array, degree=3)

# For x2_array, y2_array
print("\nLinear Regression for data2 (x2_array, y2_array)")
beta_linear2 = QR_decomposition(x2_array, y2_array, degree=1)

print("Quadratic Regression for data2 (x2_array, y2_array)")
beta_quadratic2 = QR_decomposition(x2_array, y2_array, degree=2)

print("Cubic Regression for data2 (x2_array, y2_array)")
beta_cubic2 = QR_decomposition(x2_array, y2_array, degree=3)

# For x3_array, y3_array
print("\nLinear Regression for data3 (x3_array, y3_array)")
beta_linear3 = QR_decomposition(x3_array, y3_array, degree=1)

print("Quadratic Regression for data3 (x3_array, y3_array)")
beta_quadratic3 = QR_decomposition(x3_array, y3_array, degree=2)

print("Cubic Regression for data3 (x3_array, y3_array)")
beta_cubic3 = QR_decomposition(x3_array, y3_array, degree=3)

import numpy as np
import matplotlib.pyplot as plt

def svd_solve(X, y):

    # Perform SVD decomposition of X
    U, S, Vt = np.linalg.svd(X, full_matrices=False)

    # Compute the pseudo-inverse of S
    S_inv = np.diag(1 / S)

    # Solve for the regression coefficients (beta)
    beta = Vt.T.dot(S_inv).dot(U.T).dot(y)

    return beta

def svd_regression(X, y, degree=1):

    # Adding polynomial terms (degree 1 is linear, degree 2 is quadratic, degree 3 is cubic, etc.)
    X_b = np.ones((X.shape[0], 1))  # Start with the intercept term
    for d in range(1, degree + 1):
        X_b = np.c_[X_b, X**d]

    # Solving for regression coefficients using SVD
    beta = svd_solve(X_b, y)

    # Printing the coefficients
    print(f"Coefficients: {beta}")

    # Ploting the data and the polynomial regression line
    plt.scatter(X, y, color='blue', label='Data points')

    # Ploting the fitted polynomial curve
    X_range = np.linspace(X.min(), X.max(), 500).reshape(-1, 1)
    X_range_b = np.ones((X_range.shape[0], 1))  # Start with intercept term
    for d in range(1, degree + 1):
        X_range_b = np.c_[X_range_b, X_range**d]

    plt.plot(X_range, X_range_b.dot(beta), color='red', label=f'{degree}-degree Polynomial Fit')
    plt.xlabel('X')
    plt.ylabel('y')
    plt.legend()
    plt.show()

    return beta

# Example Usage for x1_array, y1_array, x2_array, y2_array, and x3_array, y3_array

# For x1_array, y1_array
print("Linear Regression for data1 (x1_array, y1_array)")
beta_linear = svd_regression(x1_array, y1_array, degree=1)

print("Quadratic Regression for data1 (x1_array, y1_array)")
beta_quadratic = svd_regression(x1_array, y1_array, degree=2)

print("Cubic Regression for data1 (x1_array, y1_array)")
beta_cubic = svd_regression(x1_array, y1_array, degree=3)

# For x2_array, y2_array
print("\nLinear Regression for data2 (x2_array, y2_array)")
beta_linear2 = svd_regression(x2_array, y2_array, degree=1)

print("Quadratic Regression for data2 (x2_array, y2_array)")
beta_quadratic2 = svd_regression(x2_array, y2_array, degree=2)

print("Cubic Regression for data2 (x2_array, y2_array)")
beta_cubic2 = svd_regression(x2_array, y2_array, degree=3)

# For x3_array, y3_array
print("\nLinear Regression for data3 (x3_array, y3_array)")
beta_linear3 = svd_regression(x3_array, y3_array, degree=1)

print("Quadratic Regression for data3 (x3_array, y3_array)")
beta_quadratic3 = svd_regression(x3_array, y3_array, degree=2)

print("Cubic Regression for data3 (x3_array, y3_array)")
beta_cubic3 = svd_regression(x3_array, y3_array, degree=3)